# Video Scene Splitter Configuration File
# This file contains all configurable parameters for the video scene splitting scripts
# Modify these values to customize the behavior of the scene detection and encoding

[general]
# Default output directory pattern (use {input_stem} for input filename without extension)
default_output_pattern = {input_stem}_scenes

# Default batch size for batch processing (frames per batch)
default_batch_size = 100

# Default maximum frames per scene before forced split
default_max_frames = 1000

# Default scene detection threshold (lower = more sensitive, more scenes)
default_threshold = 30.0

# Whether to create scene list files by default
create_scene_list = true

# Whether to merge adjacent/overlapping scenes
merge_adjacent_scenes = true

# Tolerance for merging adjacent scenes (seconds)
merge_tolerance = 0.1

[scene_detection]
# Scene detection algorithm parameters
threshold = 27.0
min_scene_len = 15
fade_bias = 0.0

# Content detector specific settings
luma_only = false
kernel_size = 3

# Performance and accuracy settings
# detector_type: Options are content, adaptive, histogram, hash, threshold, multi
detector_type = content
# use_stats_manager: WARNING - Enabling stats can give false performance readings for repeated videos
use_stats_manager = false
# enable_downscaling: Enable frame downscaling for better performance
enable_downscaling = true
# downscale_factor: 0 = auto, 1 = no downscale, 2+ = downscale factor
downscale_factor = 0

# Multi-detector settings (when detector_type = multi)
multi_detector_threshold = 1.0
content_weight = 1.0
adaptive_weight = 0.8
histogram_weight = 0.6
hash_weight = 0.9

# Advanced detector parameters
adaptive_window_frames = 60
histogram_bins = 256
hash_size = 8

# Interpolation method for frame scaling/processing
# interpolation_method: Options are nearest, linear, cubic, area, lanczos4
interpolation_method = linear

# Note: event_buffer_length is no longer configurable in PySceneDetect 0.6+
# Each detector algorithm automatically determines its optimal buffer length

[video]
# Video backend selection
# backend: Options are auto, opencv, pyav, moviepy
backend = auto
# Video input parameters
skip_frames = 0
max_duration = 0

# Frame extraction settings
frame_skip = 1
downscale_factor = 1.0

[encoding]
# Video encoding parameters
video_codec = libsvtav1
audio_codec = libopus

# Video quality settings
crf = 30
preset = 6
pixel_format = yuv420p
color_range = tv
color_space = bt709

# Audio settings
audio_bitrate = 128k
audio_sample_rate = 48000
audio_channels = 2

# Advanced encoding options
two_pass = false
tune = 
profile = 

# Container settings
container_format = mp4
faststart = true

[ffmpeg]
# FFmpeg-specific settings
ffmpeg_binary = ffmpeg
ffmpeg_timeout = 300
ffmpeg_threads = 0

# Error handling
continue_on_error = false
retry_failed = 1

# Additional FFmpeg arguments (advanced)
extra_input_args = []
extra_output_args = []

[output]
# Output file naming
scene_filename_pattern = scene_{number:04d}.{extension}
scene_list_filename = scene_list.txt
metadata_filename = metadata.json

# File organization
organize_by_duration = false
organize_by_size = false
duration_thresholds = [5, 15, 60]
size_thresholds = [1, 10, 50]

# Metadata export
export_metadata = true
include_timestamps = true
include_file_info = true
include_encoding_info = true

[batch_processing]
# Batch processing specific settings
# batch_size: Frames per batch
batch_size = 100
# overlap_frames: Overlap between batches to catch scene transitions
overlap_frames = 10
# memory_limit_mb: Memory limit for batch processing (MB)
memory_limit_mb = 1024
# parallel_batches: Number of batches to process in parallel
parallel_batches = 1

# Batch optimization
# adaptive_batch_size: Automatically adjust batch size based on video
adaptive_batch_size = true
# min_batch_size: Minimum batch size when adaptive
min_batch_size = 50
# max_batch_size: Maximum batch size when adaptive
max_batch_size = 500

[quality_control]
# Quality control and validation
# min_scene_duration: Minimum scene duration in seconds
min_scene_duration = 0.5
# max_scene_duration: Maximum scene duration in seconds (0 = no limit)
max_scene_duration = 300
# min_file_size_kb: Minimum output file size in KB
min_file_size_kb = 10
# max_file_size_mb: Maximum output file size in MB (0 = no limit)
max_file_size_mb = 1000

# Validation checks
# verify_output_files: Verify output files after creation
verify_output_files = true
# check_video_integrity: Check video integrity with ffprobe
check_video_integrity = true
# remove_invalid_files: Remove files that fail validation
remove_invalid_files = false

[logging]
# Logging and progress reporting
log_level = INFO
log_to_file = true
log_filename_pattern = {input_stem}_processing.log
progress_bar = true
verbose_ffmpeg = false

# Statistics
collect_statistics = true
statistics_filename = processing_stats.json

[performance]
# Performance optimization
# use_gpu_acceleration: Use GPU acceleration if available
use_gpu_acceleration = false
# gpu_device: GPU device index
gpu_device = 0
# memory_optimization: Enable memory optimizations
memory_optimization = true
# disk_cache_size_mb: Disk cache size for temporary files
disk_cache_size_mb = 512

# Threading
# max_workers: Maximum number of worker threads
max_workers = 4
# io_threads: Number of I/O threads
io_threads = 2
# encoding_threads: Number of encoding threads per job (0 = auto)
encoding_threads = 0

[advanced]
# Advanced processing options
# custom_scene_detectors: Custom scene detector configurations
custom_scene_detectors = []
# post_processing_filters: Post-processing video filters
post_processing_filters = []
# scene_transition_effects: Transition effects between scenes
scene_transition_effects = []

# Experimental features
# experimental_features: Enable experimental features
experimental_features = false
# debug_mode: Enable debug mode
debug_mode = false
# dry_run: Perform dry run without actual encoding
dry_run = false

# Integration settings
# webhook_url: Webhook URL for completion notifications
webhook_url = ""
# email_notifications: Send email notifications
email_notifications = false
# notification_events: Events to notify about
notification_events = ["completion", "error"]

[segmentation]
# YOLO model configuration
yolo_model = yoloe-11l-seg-pf.pt
# confidence_threshold: Minimum confidence for object detection (YOLO's built-in filter)
confidence_threshold = 0.45
# iou_threshold: IoU threshold for non-maximum suppression (YOLO's built-in filter)
iou_threshold = 0.7
# max_objects_per_frame: Maximum detections per frame (YOLO's max_det parameter)
max_objects_per_frame = 2
device = cuda
# classes: YOLO class IDs to include (empty = all classes, e.g., [0, 2, 5])
classes = []

# Performance optimization settings
# yolo_image_size: YOLO's built-in image resizing (imgsz parameter)
# Can be a single integer (320, 640) for square resizing or (height, width) tuple
yolo_image_size = 320
# yolo_half_precision: Use half precision (FP16) for faster inference
yolo_half_precision = true
# yolo_stream: Use streaming mode for memory efficiency
yolo_stream = true

[object_tracking]
# Two-stage filtering process:
# 1. YOLO's built-in filtering: confidence_threshold, iou_threshold, max_objects_per_frame, classes
# 2. Our custom strategy: further selection from YOLO's filtered results

# Custom selection strategy (applied after YOLO's filtering)
# selection_strategy: Options are confidence, size, center, area
selection_strategy = confidence
# min_object_area: Minimum object area in pixels (additional filter)
min_object_area = 500
# frame_skip: Process every Nth frame (1=all frames, 2=every other frame, 3=every third frame)
frame_skip = 2

# Note: Classes are filtered by YOLO using class IDs in the 'classes' parameter above
# Common YOLO class IDs: 0=person, 1=bicycle, 2=car, 3=motorcycle, 5=bus, 7=truck
# Leave 'classes' empty to detect all object types

[background_processing]
# Background processing method - unified system with four options:
# 'none' - No processing, use original frames as background
# 'masking' - Overlay mask on objects (shows what would be inpainted)
# 'opencv_inpaint' - Traditional OpenCV inpainting (cv2.inpaint)
# 'ai_inpaint' - AI-powered Stable Diffusion inpainting
background_method = opencv_inpaint

# OpenCV inpainting settings (for opencv_inpaint method)
opencv_inpaint_method = telea
opencv_inpaint_radius = 3

# AI inpainting settings (for ai_inpaint method)
ai_inpaint_model = stabilityai/stable-diffusion-2-inpainting
ai_inpaint_prompt = clean background, natural lighting, high quality
ai_inpaint_guidance_scale = 7.5
ai_inpaint_num_inference_steps = 20

# Mask processing settings (applies to all methods that use masks)
dilate_mask = true
dilate_kernel_size = 3

# Masking visualization settings (for masking method)
mask_color = [255, 0, 0]  # RGB color for mask overlay (red by default)
mask_alpha = 0.5  # Transparency of mask overlay (0.0 = transparent, 1.0 = opaque)

[object_output]
# Object and background output settings
save_individual_objects = true
save_combined_mask = true
save_inpainted_background = true
object_image_format = png
background_image_format = png

[panorama]
# Panorama stitching configuration
# stitching_method: Choose panorama stitching method - 'opencv' or 'stitching_lib'
stitching_method = opencv
# stitcher_mode: OpenCV stitcher mode - 'panorama' for general scenes, 'scans' for planar scenes (only for opencv method)
stitcher_mode = panorama
# max_frames: Maximum number of frames to use for stitching (for performance)
max_frames = 20
# downsample_factor: Factor to downsample frames for initial stitching (0.5 = half size)
downsample_factor = 0.5
# fallback_to_median: Whether to fall back to median composite if stitching fails
fallback_to_median = true

# Stitching library specific settings (only for stitching_lib method)
# detector: Feature detector for stitching library - 'sift', 'orb', 'surf', etc.
stitching_detector = sift
# confidence_threshold: Confidence threshold for stitching library
stitching_confidence_threshold = 0.2
